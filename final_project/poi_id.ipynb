{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data,test_classifier\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectPercentile,SelectKBest\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load data\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total number of records 146\n",
      "the total number of features 20\n",
      "the names of features: {'salary': 365788, 'to_messages': 807, 'deferral_payments': 'NaN', 'total_payments': 1061827, 'exercised_stock_options': 'NaN', 'bonus': 600000, 'restricted_stock': 585062, 'shared_receipt_with_poi': 702, 'restricted_stock_deferred': 'NaN', 'total_stock_value': 585062, 'expenses': 94299, 'loan_advances': 'NaN', 'from_messages': 29, 'other': 1740, 'from_this_person_to_poi': 1, 'poi': False, 'director_fees': 'NaN', 'deferred_income': 'NaN', 'long_term_incentive': 'NaN', 'email_address': 'mark.metts@enron.com', 'from_poi_to_this_person': 38}\n",
      "the total number of poi: 18\n"
     ]
    }
   ],
   "source": [
    "print \"the total number of records\",len(data_dict)\n",
    "\n",
    "for record in data_dict.values():\n",
    "    print \"the total number of features\", len(record)-1\n",
    "    print \"the names of features:\", record    \n",
    "    break\n",
    "\n",
    "poi_records = []\n",
    "\n",
    "for record in data_dict.items():    \n",
    "    if record[1]['poi'] == True:\n",
    "        poi_records.append(record)\n",
    "        \n",
    "\n",
    "    \n",
    "print \"the total number of poi:\", len(poi_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look through the dataset to see the 'NaN'conditions of different features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN numbers for each feature:\n",
      "poi 0 \n",
      "\n",
      "salary 51 \n",
      "\n",
      "bonus 64 \n",
      "\n",
      "total_payments 21 \n",
      "\n",
      "deferral_payments 107 \n",
      "\n",
      "exercised_stock_options 44 \n",
      "\n",
      "restricted_stock 36 \n",
      "\n",
      "restricted_stock_deferred 128 \n",
      "\n",
      "total_stock_value 20 \n",
      "\n",
      "expenses 51 \n",
      "\n",
      "other 53 \n",
      "\n",
      "director_fees 129 \n",
      "\n",
      "loan_advances 142 \n",
      "\n",
      "deferred_income 97 \n",
      "\n",
      "long_term_incentive 80 \n",
      "\n",
      "from_poi_to_this_person 60 \n",
      "\n",
      "from_this_person_to_poi 60 \n",
      "\n",
      "to_messages 60 \n",
      "\n",
      "from_messages 60 \n",
      "\n",
      "shared_receipt_with_poi 60 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_list = ['poi','salary','bonus','total_payments','deferral_payments','exercised_stock_options',\\\n",
    "                 'restricted_stock','restricted_stock_deferred','total_stock_value','expenses',\\\n",
    "                 'other','director_fees','loan_advances','deferred_income','long_term_incentive',\\\n",
    "                 'from_poi_to_this_person','from_this_person_to_poi','to_messages','from_messages',\\\n",
    "                 'shared_receipt_with_poi']\n",
    "\n",
    "print \"NaN numbers for each feature:\"\n",
    "\n",
    "for feature in features_list:\n",
    "    nan_num = 0\n",
    "    for record in data_dict.items():    \n",
    "        for key, value in record[1].items():\n",
    "            if feature == key and value == \"NaN\":\n",
    "                nan_num += 1\n",
    "    print feature, nan_num, \"\\n\"\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for deferral_payments, restricted_stock_deferred, director_fees and loan_advances, There are more than 100 reacords are \"NaN\". We will not use these 4 features when modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find outliers in the data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We need to find records which have unreasonable feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2c25bb5fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for record in data_dict.items():\n",
    "    plt.scatter(record[1][\"salary\"], record[1][\"bonus\"])\n",
    "    \n",
    "plt.xlabel(\"bonus\")\n",
    "plt.ylabel(\"salary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are records where salary and bonus are extreamlly high. we need find these records and remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26704229, 1111258, 1072321]\n",
      "[97343619, 8000000, 7000000]\n"
     ]
    }
   ],
   "source": [
    "salary_outliers = []\n",
    "bonus_outliers = []\n",
    "\n",
    "for record in data_dict.items():\n",
    "    salary = record[1]['salary']\n",
    "    if salary == \"NaN\":\n",
    "        continue\n",
    "    salary_outliers.append(salary)\n",
    "    \n",
    "for record in data_dict.items():       \n",
    "    bonus = record[1]['bonus']\n",
    "    if bonus == \"NaN\":\n",
    "        continue\n",
    "    bonus_outliers.append(bonus)\n",
    "\n",
    "#sort to find the max 3 ones in each list\n",
    "salary_outliers.sort(reverse = True)\n",
    "bonus_outliers.sort(reverse = True)\n",
    "\n",
    "print salary_outliers[:3]\n",
    "print bonus_outliers[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can find that the max one in each list is significantlly higher than others. We assume that the record which contains above max values is outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL\n",
      "TOTAL\n"
     ]
    }
   ],
   "source": [
    "for record in data_dict.items():\n",
    "    if record[1]['salary'] == salary_outliers[0]:\n",
    "        print record[0]\n",
    "        break\n",
    "        \n",
    "for record in data_dict.items():\n",
    "    if record[1]['bonus'] == bonus_outliers[0]:\n",
    "        print record[0]\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For some records with too much NaN features, we can assum they may are not hunman related records and need to be removed. Also, if all features in a record are NaN, it is a unvaluble record and should be remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHALEY DAVID A , NaN_num: 18\n",
      "WROBEL BRUCE , NaN_num: 18\n",
      "LOCKHART EUGENE E , NaN_num: 20\n",
      "THE TRAVEL AGENCY IN THE PARK , NaN_num: 18\n",
      "GRAMM WENDY L , NaN_num: 18\n"
     ]
    }
   ],
   "source": [
    "for record in data_dict.items():\n",
    "    NaN_num = 0\n",
    "    for feature in record[1].items():\n",
    "        if feature[1] == \"NaN\":\n",
    "            NaN_num += 1\n",
    "    if 17 < NaN_num < 20:\n",
    "        print record[0], \", NaN_num:\", NaN_num  \n",
    "    if NaN_num == 20:\n",
    "        print record[0], \", NaN_num:\", NaN_num  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is obvious to see that \"THE TRAVEL AGENCY IN THE PARK\" is not a person and all features in the record with \"LOCKHART EUGENE E\" key are NaN. we remove above outliers in the next step.\n",
    "# Remove outliners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict.pop(\"TOTAL\")\n",
    "data_dict.pop(\"THE TRAVEL AGENCY IN THE PARK\")\n",
    "data_dict.pop(\"LOCKHART EUGENE E\")\n",
    "\n",
    "my_dataset = data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add new features\n",
    "we assum that suspects contact with poi frequetly. Therefore we introduce the features \"from_poi_ratio\" and \"to_poi_ratio\".\n",
    "\n",
    "    to_ratio = from_poi_to_this_person/to_messages;\n",
    "    from_ratio = from_this_person_to_poi/from_messages;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in data_dict:\n",
    "    if data_dict[key]['from_poi_to_this_person'] == \"NaN\" or data_dict[key]['to_messages'] == \"NaN\":\n",
    "        data_dict[key]['to_ratio'] = \"NaN\"\n",
    "    else:\n",
    "        data_dict[key]['to_ratio'] = float(data_dict[key]['from_poi_to_this_person']/data_dict[key]['to_messages'])     \n",
    "        \n",
    "    if data_dict[key]['from_this_person_to_poi'] == \"NaN\" or data_dict[key]['from_messages'] == \"NaN\":\n",
    "        data_dict[key]['from_ratio'] = \"NaN\"\n",
    "    else:\n",
    "        data_dict[key]['from_ratio'] = float(data_dict[key]['from_this_person_to_poi']/data_dict[key]['from_messages']) \n",
    "\n",
    "\n",
    "features_list = ['poi','salary','bonus','total_payments','deferral_payments','exercised_stock_options',\\\n",
    "                 'restricted_stock','restricted_stock_deferred','total_stock_value','expenses',\\\n",
    "                 'other','director_fees','loan_advances','deferred_income','long_term_incentive',\\\n",
    "                 'from_poi_to_this_person','from_this_person_to_poi','to_messages','from_messages',\\\n",
    "                 'shared_receipt_with_poi','to_ratio','from_ratio']\n",
    "\n",
    "my_dataset = data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection and scalling\n",
    "#### use SelectKBest algorithm to select best 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected_features: [('exercised_stock_options', 24.815079733218194), ('total_stock_value', 24.18289867856688), ('bonus', 20.792252047181535), ('salary', 18.289684043404513), ('deferred_income', 11.458476579280369)]\n",
      "selected_features_4: [('exercised_stock_options', 24.815079733218194), ('total_stock_value', 24.18289867856688), ('bonus', 20.792252047181535), ('salary', 18.289684043404513)]\n",
      "selected_features_6: [('exercised_stock_options', 24.815079733218194), ('total_stock_value', 24.18289867856688), ('bonus', 20.792252047181535), ('salary', 18.289684043404513), ('deferred_income', 11.458476579280369), ('long_term_incentive', 9.922186013189823)]\n",
      "my_features_list: ['poi', 'exercised_stock_options', 'total_stock_value', 'bonus', 'salary', 'deferred_income']\n",
      "my_features_list_3: ['poi', 'exercised_stock_options', 'total_stock_value', 'bonus']\n"
     ]
    }
   ],
   "source": [
    "def Select_K_Best(data_dict, features_list, k):\n",
    "    data = featureFormat(data_dict, features_list)\n",
    "    labels, features = targetFeatureSplit(data)\n",
    "    k_best_modal = SelectKBest(k=k)\n",
    "    k_best_modal.fit(features, labels)\n",
    "    scores = k_best_modal.scores_\n",
    "    tuples_unsorted = zip(features_list[1:], scores)\n",
    "    k_best_features = sorted(tuples_unsorted, key=lambda x: x[1], reverse=True)   \n",
    "    return k_best_features[:k]\n",
    "\n",
    "selected_features = Select_K_Best(my_dataset,features_list,5)\n",
    "selected_features_4 = Select_K_Best(my_dataset,features_list,4)\n",
    "selected_features_6 = Select_K_Best(my_dataset,features_list,6)\n",
    "print \"selected_features:\", selected_features\n",
    "print \"selected_features_4:\", selected_features_4\n",
    "print \"selected_features_6:\", selected_features_6\n",
    "\n",
    "my_features_list = ['poi'] \n",
    "my_features_list_4 = ['poi'] \n",
    "my_features_list_6 = ['poi'] \n",
    "for feature in selected_features:\n",
    "    my_features_list = my_features_list + [feature[0]]\n",
    "for feature in selected_features_4:\n",
    "    my_features_list_4 = my_features_list_4 + [feature[0]]\n",
    "for feature in selected_features_6:\n",
    "    my_features_list_6 = my_features_list_6 + [feature[0]]\n",
    "    \n",
    "print \"my_features_list:\",my_features_list\n",
    "print \"my_features_list_3:\",my_features_list_3\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, my_features_list, sort_keys = True)\n",
    "data_4 = featureFormat(my_dataset, my_features_list_4, sort_keys = True)\n",
    "data_6 = featureFormat(my_dataset, my_features_list_6, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "labels_4, features_4 = targetFeatureSplit(data_4)\n",
    "labels_6, features_6 = targetFeatureSplit(data_6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we normalize data by feature scalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "features_new = scaler.fit_transform(features)\n",
    "features_new_4 = scaler.fit_transform(features_4)\n",
    "features_new_6 = scaler.fit_transform(features_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try a varity of classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### divide data into training data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.asarray(labels)\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features_new, labels, test_size=0.3, random_state=42)\n",
    "features_train_4, features_test_4, labels_train_4, labels_test_4 = \\\n",
    "    train_test_split(features_new_4, labels_4, test_size=0.3, random_state=42)\n",
    "features_train_6, features_test_6, labels_train_6, labels_test_6 = \\\n",
    "    train_test_split(features_new_6, labels_6, test_size=0.3, random_state=42)\n",
    "    \n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from time import time\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9047619047619048\n",
      "Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('gnb', GaussianNB(priors=None))])\n",
      "\tAccuracy: 0.84300\tPrecision: 0.48581\tRecall: 0.35100\tF1: 0.40755\tF2: 0.37163\n",
      "\tTotal predictions: 13000\tTrue positives:  702\tFalse positives:  743\tFalse negatives: 1298\tTrue negatives: 10257\n",
      "\n",
      "None\n",
      "time consuming:  0.0159511566162\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "begin = time()\n",
    "\n",
    "clf_NB = GaussianNB()\n",
    "parm = {}\n",
    "clf_NB = Pipeline([('scaler',scaler),('gnb',clf_NB)])\n",
    "gs = GridSearchCV(clf_NB, parm)\n",
    "gs.fit(features_train,labels_train)\n",
    "clf_NB = gs.best_estimator_\n",
    "pred = clf_NB.predict(features_test)\n",
    "\n",
    "end = time()\n",
    "\n",
    "accuracy = accuracy_score(labels_test,pred)\n",
    "\n",
    "print accuracy\n",
    "print test_classifier(clf_NB,my_dataset,my_features_list_3)\n",
    "print \"time consuming: \", end - begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to see how many features are best for modalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "select top 4 features\n",
      "0.8974358974358975\n",
      "Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('gnb', GaussianNB(priors=None))])\n",
      "\tAccuracy: 0.84677\tPrecision: 0.50312\tRecall: 0.32300\tF1: 0.39342\tF2: 0.34791\n",
      "\tTotal predictions: 13000\tTrue positives:  646\tFalse positives:  638\tFalse negatives: 1354\tTrue negatives: 10362\n",
      "\n",
      "None\n",
      "time consuming:  0.0128149986267\n",
      "\n",
      "select top 6 features\n",
      "0.9285714285714286\n",
      "Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('gnb', GaussianNB(priors=None))])\n",
      "\tAccuracy: 0.84714\tPrecision: 0.45679\tRecall: 0.37000\tF1: 0.40884\tF2: 0.38462\n",
      "\tTotal predictions: 14000\tTrue positives:  740\tFalse positives:  880\tFalse negatives: 1260\tTrue negatives: 11120\n",
      "\n",
      "None\n",
      "time consuming:  0.012885093689\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print \"\\n\",\"select top 4 features\"\n",
    "begin = time()\n",
    "\n",
    "clf_NB = GaussianNB()\n",
    "parm = {}\n",
    "clf_NB = Pipeline([('scaler',scaler),('gnb',clf_NB)])\n",
    "gs = GridSearchCV(clf_NB, parm)\n",
    "gs.fit(features_train_4,labels_train_4)\n",
    "clf_NB = gs.best_estimator_\n",
    "pred = clf_NB.predict(features_test_4)\n",
    "\n",
    "end = time()\n",
    "\n",
    "accuracy = accuracy_score(labels_test_4,pred)\n",
    "\n",
    "print accuracy\n",
    "print test_classifier(clf_NB,my_dataset,my_features_list_4)\n",
    "print \"time consuming: \", end - begin\n",
    "\n",
    "print \"\\n\",\"select top 6 features\"\n",
    "begin = time()\n",
    "\n",
    "clf_NB = GaussianNB()\n",
    "parm = {}\n",
    "clf_NB = Pipeline([('scaler',scaler),('gnb',clf_NB)])\n",
    "gs = GridSearchCV(clf_NB, parm)\n",
    "gs.fit(features_train_6,labels_train_6)\n",
    "clf_NB = gs.best_estimator_\n",
    "pred = clf_NB.predict(features_test_6)\n",
    "\n",
    "end = time()\n",
    "\n",
    "accuracy = accuracy_score(labels_test_6,pred)\n",
    "\n",
    "print accuracy\n",
    "print test_classifier(clf_NB,my_dataset,my_features_list_6)\n",
    "print \"time consuming: \", end - begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9523809523809523\n",
      "Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('svc', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "\tAccuracy: 0.85121\tPrecision: 0.14530\tRecall: 0.00850\tF1: 0.01606\tF2: 0.01047\n",
      "\tTotal predictions: 14000\tTrue positives:   17\tFalse positives:  100\tFalse negatives: 1983\tTrue negatives: 11900\n",
      "\n",
      "None\n",
      "time consuming:  0.0529839992523\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "begin = time()\n",
    "\n",
    "clf_SVC = SVC()\n",
    "parms = {'svc__kernel':('linear','rbf'),'svc__C':[1.0,2.0]}\n",
    "pipeline = Pipeline([('scaler',scaler),('svc',clf_SVC)])\n",
    "gs = GridSearchCV(pipeline, parms)\n",
    "gs.fit(features_train,labels_train)\n",
    "clf_SVC = gs.best_estimator_\n",
    "pred = clf_SVC.predict(features_test)\n",
    "\n",
    "end = time()\n",
    "accuracy = accuracy_score(labels_test,pred)\n",
    "\n",
    "print accuracy\n",
    "print test_classifier(clf_SVC,my_dataset,my_features_list)\n",
    "print \"time consuming: \", end - begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "begin = time()\n",
    "\n",
    "clf_RF = RandomForestClassifier()\n",
    "parms = {'criterion': ['gini', 'entropy'], \\\n",
    "         'max_depth': [None, 3, 5, 10], \\\n",
    "         'max_leaf_nodes': [None, 5, 10, 20], \\\n",
    "         'n_estimators': [1, 5, 10, 50, 100]}\n",
    "gs = GridSearchCV(clf_RF, parms)\n",
    "gs.fit(features_train,labels_train)\n",
    "clf_RF = gs.best_estimator_\n",
    "pred = clf_RF.predict(features_test)\n",
    "\n",
    "end = time()\n",
    "accuracy = accuracy_score(labels_test,pred)\n",
    "\n",
    "print accuracy\n",
    "print test_classifier(clf_RF,my_dataset,my_features_list)\n",
    "print \"time consuming: \", end - begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "begin = time()\n",
    "\n",
    "clf_AB = AdaBoostClassifier()\n",
    "parms = {'learning_rate': [0.05, 0.1, 0.5, 2.0], \\\n",
    "              'algorithm': ['SAMME', 'SAMME.R'], \\\n",
    "              'n_estimators': [1, 5, 10, 50, 100]}\n",
    "gs = GridSearchCV(clf_AB, parms)\n",
    "gs.fit(features_train,labels_train)\n",
    "clf_AB = gs.best_estimator_\n",
    "pred = clf_AB.predict(features_test)\n",
    "\n",
    "end = time()\n",
    "accuracy = accuracy_score(labels_test,pred)\n",
    "\n",
    "print accuracy\n",
    "print test_classifier(clf_AB,my_dataset,my_features_list)\n",
    "print \"time consuming: \", end - begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
